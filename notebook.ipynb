{"cells":[{"source":"![Shopping trolley in front of a laptop](./iStock-1249219777.jpg)\n\nIt's simple to buy any product with a click and have it delivered to your door. Online shopping has been rapidly evolving over the last few years, making our lives easier. But behind the scenes, e-commerce companies face a complex challenge that needs to be addressed. \n\nUncertainty plays a big role in how the supply chains plan and organize their operations to ensure that the products are delivered on time. These uncertainties can lead to challenges such as stockouts, delayed deliveries, and increased operational costs.\n\nYou work for the Sales & Operations Planning (S&OP) team at a multinational e-commerce company. They need your help to assist in planning for the upcoming end-of-the-year sales. They want to use your insights to plan for promotional opportunities and manage their inventory. This effort is to ensure they have the right products in stock when needed and ensure their customers are satisfied with the prompt delivery to their doorstep.\n\n\n## The Data\n\nYou are provided with a sales dataset to use. A summary and preview are provided below.\n\n# Online Retail.csv\n\n| Column     | Description              |\n|------------|--------------------------|\n| `'InvoiceNo'` | A 6-digit number uniquely assigned to each transaction |\n| `'StockCode'` | A 5-digit number uniquely assigned to each distinct product |\n| `'Description'` | The product name |\n| `'Quantity'` | The quantity of each product (item) per transaction |\n| `'UnitPrice'` | Product price per unit |\n| `'CustomerID'` | A 5-digit number uniquely assigned to each customer |\n| `'Country'` | The name of the country where each customer resides |\n| `'InvoiceDate'` | The day and time when each transaction was generated `\"MM/DD/YYYY\"` |\n| `'Year'` | The year when each transaction was generated |\n| `'Month'` | The month when each transaction was generated |\n| `'Week'` | The week when each transaction was generated (`1`-`52`) |\n| `'Day'` | The day of the month when each transaction was generated (`1`-`31`) |\n| `'DayOfWeek'` | The day of the weeke when each transaction was generated <br>(`0` = Monday, `6` = Sunday) |","metadata":{},"id":"6918e18a-c248-4929-b552-7aee2057c0eb","cell_type":"markdown"},{"source":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))","metadata":{"executionCancelledAt":null,"executionTime":12934,"lastExecutedAt":1721190492146,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":59,"type":"stream"},"2":{"height":38,"type":"stream"}},"lastExecutedByKernel":"08503937-f5f0-40e4-a125-ebb34f5a1c4b"},"id":"81a07c66-a3d4-4fdd-9c3c-7b3a19b80d62","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"},{"output_type":"stream","name":"stdout","text":"24/07/17 04:28:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"}]},{"source":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year, to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = spark.read.csv(\"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime\nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(to_timestamp(col(\"InvoiceDate\"), \"MM/dd/yyyy H:mm\")))\n\n# Extract year, month, week, day, and day of week from InvoiceDate\nsales_data = sales_data.withColumn(\"Year\", year(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"Month\", month(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"Week\", weekofyear(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"Day\", dayofmonth(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\")))\n\n# Remove rows with missing CustomerID (optional, depending on the use case)\nsales_data = sales_data.na.drop(subset=[\"CustomerID\"])\n\n# Drop existing indexed columns if they exist\ncolumns_to_drop = [\"CountryIndex\", \"DescriptionIndex\"]\nfor column in columns_to_drop:\n    if column in sales_data.columns:\n        sales_data = sales_data.drop(column)\n\n# Convert categorical variables to numerical\nindexer = StringIndexer(inputCols=[\"Country\", \"Description\"], outputCols=[\"CountryIndex\", \"DescriptionIndex\"])\nsales_data = indexer.fit(sales_data).transform(sales_data)\n\n# Select features and label\nassembler = VectorAssembler(\n    inputCols=[\"CountryIndex\", \"DescriptionIndex\", \"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\", \"UnitPrice\"],\n    outputCol=\"features\"\n)\nsales_data = assembler.transform(sales_data)\n\n# Use total quantity sold as the label for demand forecasting\nsales_data = sales_data.withColumnRenamed(\"Quantity\", \"label\")\n\n# Split data into training and test sets\ntrain_data, test_data = sales_data.randomSplit([0.8, 0.2], seed=42)\n\n# Define the RandomForestRegressor model with increased maxBins\nrf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=5000)\n\n# Create a pipeline\npipeline = Pipeline(stages=[rf])\n\n# Train the model\nmodel = pipeline.fit(train_data)\n\n# Make predictions\npredictions = model.transform(test_data)\n\n# Evaluate the model\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Save the model (optional)\nmodel.save(\"demand_forecasting_model\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":27566,"lastExecutedAt":1721191169304,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year, to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = spark.read.csv(\"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime\nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(to_timestamp(col(\"InvoiceDate\"), \"MM/dd/yyyy H:mm\")))\n\n# Extract year, month, week, day, and day of week from InvoiceDate\nsales_data = sales_data.withColumn(\"Year\", year(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"Month\", month(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"Week\", weekofyear(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"Day\", dayofmonth(col(\"InvoiceDate\")))\nsales_data = sales_data.withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\")))\n\n# Remove rows with missing CustomerID (optional, depending on the use case)\nsales_data = sales_data.na.drop(subset=[\"CustomerID\"])\n\n# Drop existing indexed columns if they exist\ncolumns_to_drop = [\"CountryIndex\", \"DescriptionIndex\"]\nfor column in columns_to_drop:\n    if column in sales_data.columns:\n        sales_data = sales_data.drop(column)\n\n# Convert categorical variables to numerical\nindexer = StringIndexer(inputCols=[\"Country\", \"Description\"], outputCols=[\"CountryIndex\", \"DescriptionIndex\"])\nsales_data = indexer.fit(sales_data).transform(sales_data)\n\n# Select features and label\nassembler = VectorAssembler(\n    inputCols=[\"CountryIndex\", \"DescriptionIndex\", \"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\", \"UnitPrice\"],\n    outputCol=\"features\"\n)\nsales_data = assembler.transform(sales_data)\n\n# Use total quantity sold as the label for demand forecasting\nsales_data = sales_data.withColumnRenamed(\"Quantity\", \"label\")\n\n# Split data into training and test sets\ntrain_data, test_data = sales_data.randomSplit([0.8, 0.2], seed=42)\n\n# Define the RandomForestRegressor model with increased maxBins\nrf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=5000)\n\n# Create a pipeline\npipeline = Pipeline(stages=[rf])\n\n# Train the model\nmodel = pipeline.fit(train_data)\n\n# Make predictions\npredictions = model.transform(test_data)\n\n# Evaluate the model\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Save the model (optional)\nmodel.save(\"demand_forecasting_model\")","lastExecutedByKernel":"08503937-f5f0-40e4-a125-ebb34f5a1c4b","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":38,"type":"stream"},"5":{"height":38,"type":"stream"},"6":{"height":38,"type":"stream"},"7":{"height":59,"type":"stream"},"8":{"height":38,"type":"stream"}}},"id":"b5106e04-f9da-459f-a1cc-14e437fe001d","cell_type":"code","execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"24/07/17 04:39:14 WARN DAGScheduler: Broadcasting large task binary with size 1692.5 KiB\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"24/07/17 04:39:16 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"Root Mean Squared Error (RMSE): 7.698109107655913\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"24/07/17 04:39:27 WARN TaskSetManager: Stage 32 contains a task of very large size (1971 KiB). The maximum recommended task size is 1000 KiB.\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"}]},{"source":"from pyspark.sql import SparkSession\nfrom pyspark.ml import PipelineModel\nfrom pyspark.sql.functions import col, to_date, to_timestamp, year, month, weekofyear, dayofmonth, dayofweek\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Load the saved model\nmodel = PipelineModel.load(\"demand_forecasting_model\")\n\n# Prepare new data for prediction\n# Here we assume new_data is a DataFrame with the same structure as the training data\n# For demonstration, let's create a new DataFrame with the same structure\nnew_data = spark.createDataFrame([\n    (0, \"United Kingdom\", \"WHITE HANGING HEART T-LIGHT HOLDER\", \"2023-07-15\", 2.55, 1)\n], [\"CustomerID\", \"Country\", \"Description\", \"InvoiceDate\", \"UnitPrice\", \"Quantity\"])\n\n# Convert InvoiceDate to datetime\nnew_data = new_data.withColumn(\"InvoiceDate\", to_date(to_timestamp(col(\"InvoiceDate\"), \"yyyy-MM-dd\")))\n\n# Extract year, month, week, day, and day of week from InvoiceDate\nnew_data = new_data.withColumn(\"Year\", year(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"Month\", month(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"Week\", weekofyear(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"Day\", dayofmonth(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\")))\n\n# Show the new data after transformations\nnew_data.show()\n\n# Check for null values in the new data\nnew_data.select([col(c).isNull().alias(c) for c in new_data.columns]).show()\n\n# Convert categorical variables to numerical using StringIndexer\nindexer = StringIndexer(inputCols=[\"Country\", \"Description\"], outputCols=[\"CountryIndex\", \"DescriptionIndex\"])\nindexed_data = indexer.fit(new_data).transform(new_data)\n\n# Show the indexed data\nindexed_data.show()\n\n# Assemble features with handleInvalid=\"skip\"\nassembler = VectorAssembler(\n    inputCols=[\"CountryIndex\", \"DescriptionIndex\", \"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\", \"UnitPrice\"],\n    outputCol=\"features\",\n    handleInvalid=\"skip\"\n)\nassembled_data = assembler.transform(indexed_data)\n\n# Show the assembled data\nassembled_data.select(\"features\", \"CountryIndex\", \"DescriptionIndex\", \"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\", \"UnitPrice\").show()\n\n# Make predictions using the loaded model\npredictions = model.transform(assembled_data)\n\n# Show predictions\npredictions.select(\"features\", \"prediction\").show()","metadata":{"executionCancelledAt":null,"executionTime":5292,"lastExecutedAt":1721191621310,"lastExecutedByKernel":"08503937-f5f0-40e4-a125-ebb34f5a1c4b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import SparkSession\nfrom pyspark.ml import PipelineModel\nfrom pyspark.sql.functions import col, to_date, to_timestamp, year, month, weekofyear, dayofmonth, dayofweek\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Load the saved model\nmodel = PipelineModel.load(\"demand_forecasting_model\")\n\n# Prepare new data for prediction\n# Here we assume new_data is a DataFrame with the same structure as the training data\n# For demonstration, let's create a new DataFrame with the same structure\nnew_data = spark.createDataFrame([\n    (0, \"United Kingdom\", \"WHITE HANGING HEART T-LIGHT HOLDER\", \"2023-07-15\", 2.55, 1)\n], [\"CustomerID\", \"Country\", \"Description\", \"InvoiceDate\", \"UnitPrice\", \"Quantity\"])\n\n# Convert InvoiceDate to datetime\nnew_data = new_data.withColumn(\"InvoiceDate\", to_date(to_timestamp(col(\"InvoiceDate\"), \"yyyy-MM-dd\")))\n\n# Extract year, month, week, day, and day of week from InvoiceDate\nnew_data = new_data.withColumn(\"Year\", year(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"Month\", month(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"Week\", weekofyear(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"Day\", dayofmonth(col(\"InvoiceDate\")))\nnew_data = new_data.withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\")))\n\n# Show the new data after transformations\nnew_data.show()\n\n# Check for null values in the new data\nnew_data.select([col(c).isNull().alias(c) for c in new_data.columns]).show()\n\n# Convert categorical variables to numerical using StringIndexer\nindexer = StringIndexer(inputCols=[\"Country\", \"Description\"], outputCols=[\"CountryIndex\", \"DescriptionIndex\"])\nindexed_data = indexer.fit(new_data).transform(new_data)\n\n# Show the indexed data\nindexed_data.show()\n\n# Assemble features with handleInvalid=\"skip\"\nassembler = VectorAssembler(\n    inputCols=[\"CountryIndex\", \"DescriptionIndex\", \"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\", \"UnitPrice\"],\n    outputCol=\"features\",\n    handleInvalid=\"skip\"\n)\nassembled_data = assembler.transform(indexed_data)\n\n# Show the assembled data\nassembled_data.select(\"features\", \"CountryIndex\", \"DescriptionIndex\", \"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\", \"UnitPrice\").show()\n\n# Make predictions using the loaded model\npredictions = model.transform(assembled_data)\n\n# Show predictions\npredictions.select(\"features\", \"prediction\").show()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":605,"type":"stream"}}},"id":"74813c4b-c8fd-4977-adaf-1db6af3dee59","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"+----------+--------------+--------------------+-----------+---------+--------+----+-----+----+---+---------+\n|CustomerID|       Country|         Description|InvoiceDate|UnitPrice|Quantity|Year|Month|Week|Day|DayOfWeek|\n+----------+--------------+--------------------+-----------+---------+--------+----+-----+----+---+---------+\n|         0|United Kingdom|WHITE HANGING HEA...| 2023-07-15|     2.55|       1|2023|    7|  28| 15|        7|\n+----------+--------------+--------------------+-----------+---------+--------+----+-----+----+---+---------+\n\n+----------+-------+-----------+-----------+---------+--------+-----+-----+-----+-----+---------+\n|CustomerID|Country|Description|InvoiceDate|UnitPrice|Quantity| Year|Month| Week|  Day|DayOfWeek|\n+----------+-------+-----------+-----------+---------+--------+-----+-----+-----+-----+---------+\n|     false|  false|      false|      false|    false|   false|false|false|false|false|    false|\n+----------+-------+-----------+-----------+---------+--------+-----+-----+-----+-----+---------+\n\n+----------+--------------+--------------------+-----------+---------+--------+----+-----+----+---+---------+------------+----------------+\n|CustomerID|       Country|         Description|InvoiceDate|UnitPrice|Quantity|Year|Month|Week|Day|DayOfWeek|CountryIndex|DescriptionIndex|\n+----------+--------------+--------------------+-----------+---------+--------+----+-----+----+---+---------+------------+----------------+\n|         0|United Kingdom|WHITE HANGING HEA...| 2023-07-15|     2.55|       1|2023|    7|  28| 15|        7|         0.0|             0.0|\n+----------+--------------+--------------------+-----------+---------+--------+----+-----+----+---+---------+------------+----------------+\n\n+--------------------+------------+----------------+----+-----+----+---+---------+---------+\n|            features|CountryIndex|DescriptionIndex|Year|Month|Week|Day|DayOfWeek|UnitPrice|\n+--------------------+------------+----------------+----+-----+----+---+---------+---------+\n|[0.0,0.0,2023.0,7...|         0.0|             0.0|2023|    7|  28| 15|        7|     2.55|\n+--------------------+------------+----------------+----+-----+----+---+---------+---------+\n\n+--------------------+------------------+\n|            features|        prediction|\n+--------------------+------------------+\n|[0.0,0.0,2023.0,7...|15.448430765525245|\n+--------------------+------------------+\n\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}